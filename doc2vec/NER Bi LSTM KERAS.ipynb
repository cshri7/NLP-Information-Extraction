{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from keras.models import Model\n",
    "from keras.layers import TimeDistributed,Conv1D,Dense,Embedding,Input,Dropout,LSTM,Bidirectional,MaxPooling1D,Flatten,concatenate\n",
    "from keras.utils import Progbar\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.initializers import RandomUniform\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "def tag_dataset(dataset):\n",
    "    correctLabels = []\n",
    "    predLabels = []\n",
    "    b = Progbar(len(dataset))\n",
    "    for i,data in enumerate(dataset):    \n",
    "        tokens, casing,char, labels = data\n",
    "        tokens = np.asarray([tokens])     \n",
    "        casing = np.asarray([casing])\n",
    "        char = np.asarray([char])\n",
    "        pred = model.predict([tokens, casing,char], verbose=False)[0]   \n",
    "        pred = pred.argmax(axis=-1) #Predict the classes            \n",
    "        correctLabels.append(labels)\n",
    "        predLabels.append(pred)\n",
    "        b.update(i)\n",
    "    return predLabels, correctLabels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>website</th>\n",
       "      <th>text</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>securelist</td>\n",
       "      <td>Analysis of the malicious CCleaner code allowe...</td>\n",
       "      <td>['APT17', 'Aurora']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>securelist</td>\n",
       "      <td>The Platinum, APT16, EvilPost and SPIVY groups...</td>\n",
       "      <td>['Platinum', 'APT16', 'EvilPost', 'SPIVY']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>securelist</td>\n",
       "      <td>This variant was also used by the APT16 group ...</td>\n",
       "      <td>['APT16', 'ELMER', 'backdoor']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>securelist</td>\n",
       "      <td>The CnC has been used in other APT incidents, ...</td>\n",
       "      <td>['admin@338', 'Temper', 'Panda']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>securelist</td>\n",
       "      <td>The TwoForOne (Platinum) group is described in...</td>\n",
       "      <td>['APT16', 'EvilPost', 'Danti']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>securelist</td>\n",
       "      <td>The Mandiant reports starts off by stating tha...</td>\n",
       "      <td>['APT1', 'Chinese']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>securelist</td>\n",
       "      <td>BlackOasis is an APT group we have been tracki...</td>\n",
       "      <td>['BlackOasis', 'APT']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>securelist</td>\n",
       "      <td>In its affidavit for sink-holing the C2, the F...</td>\n",
       "      <td>['Sofacy', 'APT28', 'Pawn', 'Storm', 'Sednit',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>securelist</td>\n",
       "      <td>Sofacy Group, also known as apt28, sandworm, x...</td>\n",
       "      <td>['Sofacy', 'apt28', 'sandworm', 'x-agent', 'pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>securelist</td>\n",
       "      <td>Sofacy (aka APT28, Fancy Bear and Tsar Team) i...</td>\n",
       "      <td>['Sofacy', 'APT28', 'Fancy', 'Bear', 'Tsar', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>securelist</td>\n",
       "      <td>Sofacy, also known as APT28, Fancy Bear, and T...</td>\n",
       "      <td>['Sofacy', 'APT28', 'Fancy', 'Bear', 'Tsar', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>securelist</td>\n",
       "      <td>Sofacy (also known as “Fancy Bear”, “Sednit”, ...</td>\n",
       "      <td>['Sofacy', 'Fancy', 'Bear', 'Sednit', 'STRONTI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>crowdstrike</td>\n",
       "      <td>Recently, in July and August 2018, IntrusionTr...</td>\n",
       "      <td>['STONE', 'PANDA', 'APT10']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>crowdstrike</td>\n",
       "      <td>Although the group’s exact motives remain uncl...</td>\n",
       "      <td>['GOTHIC', 'PANDA', 'APT3']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>crowdstrike</td>\n",
       "      <td>As an example, we all know Comment Crew, A/K/A...</td>\n",
       "      <td>['Crew', 'Team', 'APT-1', 'Comment']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>crowdstrike</td>\n",
       "      <td>This actor has many other names in the informa...</td>\n",
       "      <td>['APT-29', 'Office', 'Monkeys', 'CozyCar', 'Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>crowdstrike</td>\n",
       "      <td>In his blog, Dmitri also notes that FANCY BEAR...</td>\n",
       "      <td>['FANCY', 'BEAR', 'Sofacy', 'APT']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>crowdstrike</td>\n",
       "      <td>Although the group’s exact motives remain uncl...</td>\n",
       "      <td>['GOTHIC', 'PANDA', 'APT3']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>crowdstrike</td>\n",
       "      <td>The adversary picked in our evaluation was GOT...</td>\n",
       "      <td>['GOTHIC', 'PANDA', 'APT3']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>fireeye</td>\n",
       "      <td>The discovery and attribution of APT1 to China...</td>\n",
       "      <td>['APT1', 'PLA', 'GSD']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>fireeye</td>\n",
       "      <td>APT10 (MenuPass Group) is a Chinese cyber espi...</td>\n",
       "      <td>['APT10', 'MenuPass']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>fireeye</td>\n",
       "      <td>In July 2018, FireEye devices detected and blo...</td>\n",
       "      <td>['APT10', 'Menupass']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>fireeye</td>\n",
       "      <td>The China-based threat group FireEye tracks as...</td>\n",
       "      <td>['APT3', 'UPS']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>fireeye</td>\n",
       "      <td>Cyber espionage actors, now designated by Fire...</td>\n",
       "      <td>['APT32', 'OceanLotus']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        website                                               text  \\\n",
       "0    securelist  Analysis of the malicious CCleaner code allowe...   \n",
       "1    securelist  The Platinum, APT16, EvilPost and SPIVY groups...   \n",
       "2    securelist  This variant was also used by the APT16 group ...   \n",
       "3    securelist  The CnC has been used in other APT incidents, ...   \n",
       "4    securelist  The TwoForOne (Platinum) group is described in...   \n",
       "5    securelist  The Mandiant reports starts off by stating tha...   \n",
       "6    securelist  BlackOasis is an APT group we have been tracki...   \n",
       "7    securelist  In its affidavit for sink-holing the C2, the F...   \n",
       "8    securelist  Sofacy Group, also known as apt28, sandworm, x...   \n",
       "9    securelist  Sofacy (aka APT28, Fancy Bear and Tsar Team) i...   \n",
       "10   securelist  Sofacy, also known as APT28, Fancy Bear, and T...   \n",
       "11   securelist  Sofacy (also known as “Fancy Bear”, “Sednit”, ...   \n",
       "12  crowdstrike  Recently, in July and August 2018, IntrusionTr...   \n",
       "13  crowdstrike  Although the group’s exact motives remain uncl...   \n",
       "14  crowdstrike  As an example, we all know Comment Crew, A/K/A...   \n",
       "15  crowdstrike  This actor has many other names in the informa...   \n",
       "16  crowdstrike  In his blog, Dmitri also notes that FANCY BEAR...   \n",
       "17  crowdstrike  Although the group’s exact motives remain uncl...   \n",
       "18  crowdstrike  The adversary picked in our evaluation was GOT...   \n",
       "19      fireeye  The discovery and attribution of APT1 to China...   \n",
       "20      fireeye  APT10 (MenuPass Group) is a Chinese cyber espi...   \n",
       "21      fireeye  In July 2018, FireEye devices detected and blo...   \n",
       "22      fireeye  The China-based threat group FireEye tracks as...   \n",
       "23      fireeye  Cyber espionage actors, now designated by Fire...   \n",
       "\n",
       "                                             keywords  \n",
       "0                                ['APT17', 'Aurora']   \n",
       "1          ['Platinum', 'APT16', 'EvilPost', 'SPIVY']  \n",
       "2                      ['APT16', 'ELMER', 'backdoor']  \n",
       "3                    ['admin@338', 'Temper', 'Panda']  \n",
       "4                      ['APT16', 'EvilPost', 'Danti']  \n",
       "5                                 ['APT1', 'Chinese']  \n",
       "6                               ['BlackOasis', 'APT']  \n",
       "7   ['Sofacy', 'APT28', 'Pawn', 'Storm', 'Sednit',...  \n",
       "8   ['Sofacy', 'apt28', 'sandworm', 'x-agent', 'pa...  \n",
       "9   ['Sofacy', 'APT28', 'Fancy', 'Bear', 'Tsar', '...  \n",
       "10  ['Sofacy', 'APT28', 'Fancy', 'Bear', 'Tsar', '...  \n",
       "11  ['Sofacy', 'Fancy', 'Bear', 'Sednit', 'STRONTI...  \n",
       "12                        ['STONE', 'PANDA', 'APT10']  \n",
       "13                        ['GOTHIC', 'PANDA', 'APT3']  \n",
       "14               ['Crew', 'Team', 'APT-1', 'Comment']  \n",
       "15  ['APT-29', 'Office', 'Monkeys', 'CozyCar', 'Co...  \n",
       "16                 ['FANCY', 'BEAR', 'Sofacy', 'APT']  \n",
       "17                        ['GOTHIC', 'PANDA', 'APT3']  \n",
       "18                        ['GOTHIC', 'PANDA', 'APT3']  \n",
       "19                             ['APT1', 'PLA', 'GSD']  \n",
       "20                              ['APT10', 'MenuPass']  \n",
       "21                              ['APT10', 'Menupass']  \n",
       "22                                    ['APT3', 'UPS']  \n",
       "23                            ['APT32', 'OceanLotus']  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../data/sentences1.csv', header=None, delimiter=';', error_bad_lines=False, names=['website','text', 'keywords'])\n",
    "df.reindex(np.random.permutation(df.index))\n",
    "\n",
    "df = df[df['website'] != 'test']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from ast import literal_eval\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def normalize_text(text):\n",
    "    #norm_text = text.lower()\n",
    "    norm_text = text\n",
    "    # Replace breaks with spaces\n",
    "    norm_text = norm_text.replace('<br />', '')\n",
    "    # Pad punctuation with spaces on both sides\n",
    "    norm_text = re.sub(r\"([\\.\\\",\\(\\)!\\?;:])\", \"\\\\1\", norm_text)\n",
    "    norm_text = ' '.join(norm_text.split())\n",
    "    return norm_text\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    return \" \".join([item.lower() for item in text.split() if item not in stop])\n",
    "\n",
    "def remove_non_ascii(text):\n",
    "    return ''.join([\"\" if ord(i) < 32 or ord(i) > 126 else i for i in text])\n",
    "\n",
    "df['text'] = df['text'].apply(remove_non_ascii)\n",
    "df['text'] = df['text'].apply(normalize_text)\n",
    "#df['text'] = df['text'].apply(remove_stop_words)\n",
    "df[\"text\"] = df['text'].str.replace('[^\\w\\s]','')\n",
    "df['keywords'] = df['keywords'].apply(literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>website</th>\n",
       "      <th>text</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>securelist</td>\n",
       "      <td>Analysis of the malicious CCleaner code allowe...</td>\n",
       "      <td>[APT17, Aurora]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>securelist</td>\n",
       "      <td>The Platinum APT16 EvilPost and SPIVY groups w...</td>\n",
       "      <td>[Platinum, APT16, EvilPost, SPIVY]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>securelist</td>\n",
       "      <td>This variant was also used by the APT16 group ...</td>\n",
       "      <td>[APT16, ELMER, backdoor]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>securelist</td>\n",
       "      <td>The CnC has been used in other APT incidents a...</td>\n",
       "      <td>[admin@338, Temper, Panda]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>securelist</td>\n",
       "      <td>The TwoForOne Platinum group is described in M...</td>\n",
       "      <td>[APT16, EvilPost, Danti]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>securelist</td>\n",
       "      <td>The Mandiant reports starts off by stating tha...</td>\n",
       "      <td>[APT1, Chinese]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>securelist</td>\n",
       "      <td>BlackOasis is an APT group we have been tracki...</td>\n",
       "      <td>[BlackOasis, APT]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>securelist</td>\n",
       "      <td>In its affidavit for sinkholing the C2 the FBI...</td>\n",
       "      <td>[Sofacy, APT28, Pawn, Storm, Sednit, STRONTIUM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>securelist</td>\n",
       "      <td>Sofacy Group also known as apt28 sandworm xage...</td>\n",
       "      <td>[Sofacy, apt28, sandworm, x-agent, pawn, storm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>securelist</td>\n",
       "      <td>Sofacy aka APT28 Fancy Bear and Tsar Team is a...</td>\n",
       "      <td>[Sofacy, APT28, Fancy, Bear, Tsar, Team]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>securelist</td>\n",
       "      <td>Sofacy also known as APT28 Fancy Bear and Tsar...</td>\n",
       "      <td>[Sofacy, APT28, Fancy, Bear, Tsar, Team]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>securelist</td>\n",
       "      <td>Sofacy also known as Fancy Bear Sednit STRONTI...</td>\n",
       "      <td>[Sofacy, Fancy, Bear, Sednit, STRONTIUM, APT28]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>crowdstrike</td>\n",
       "      <td>Recently in July and August 2018 IntrusionTrut...</td>\n",
       "      <td>[STONE, PANDA, APT10]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>crowdstrike</td>\n",
       "      <td>Although the groups exact motives remain uncle...</td>\n",
       "      <td>[GOTHIC, PANDA, APT3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>crowdstrike</td>\n",
       "      <td>As an example we all know Comment Crew AKA Com...</td>\n",
       "      <td>[Crew, Team, APT-1, Comment]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>crowdstrike</td>\n",
       "      <td>This actor has many other names in the informa...</td>\n",
       "      <td>[APT-29, Office, Monkeys, CozyCar, CozyDuke]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>crowdstrike</td>\n",
       "      <td>In his blog Dmitri also notes that FANCY BEAR ...</td>\n",
       "      <td>[FANCY, BEAR, Sofacy, APT]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>crowdstrike</td>\n",
       "      <td>Although the groups exact motives remain uncle...</td>\n",
       "      <td>[GOTHIC, PANDA, APT3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>crowdstrike</td>\n",
       "      <td>The adversary picked in our evaluation was GOT...</td>\n",
       "      <td>[GOTHIC, PANDA, APT3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>fireeye</td>\n",
       "      <td>The discovery and attribution of APT1 to China...</td>\n",
       "      <td>[APT1, PLA, GSD]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>fireeye</td>\n",
       "      <td>APT10 MenuPass Group is a Chinese cyber espion...</td>\n",
       "      <td>[APT10, MenuPass]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>fireeye</td>\n",
       "      <td>In July 2018 FireEye devices detected and bloc...</td>\n",
       "      <td>[APT10, Menupass]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>fireeye</td>\n",
       "      <td>The Chinabased threat group FireEye tracks as ...</td>\n",
       "      <td>[APT3, UPS]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>fireeye</td>\n",
       "      <td>Cyber espionage actors now designated by FireE...</td>\n",
       "      <td>[APT32, OceanLotus]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        website                                               text  \\\n",
       "0    securelist  Analysis of the malicious CCleaner code allowe...   \n",
       "1    securelist  The Platinum APT16 EvilPost and SPIVY groups w...   \n",
       "2    securelist  This variant was also used by the APT16 group ...   \n",
       "3    securelist  The CnC has been used in other APT incidents a...   \n",
       "4    securelist  The TwoForOne Platinum group is described in M...   \n",
       "5    securelist  The Mandiant reports starts off by stating tha...   \n",
       "6    securelist  BlackOasis is an APT group we have been tracki...   \n",
       "7    securelist  In its affidavit for sinkholing the C2 the FBI...   \n",
       "8    securelist  Sofacy Group also known as apt28 sandworm xage...   \n",
       "9    securelist  Sofacy aka APT28 Fancy Bear and Tsar Team is a...   \n",
       "10   securelist  Sofacy also known as APT28 Fancy Bear and Tsar...   \n",
       "11   securelist  Sofacy also known as Fancy Bear Sednit STRONTI...   \n",
       "12  crowdstrike  Recently in July and August 2018 IntrusionTrut...   \n",
       "13  crowdstrike  Although the groups exact motives remain uncle...   \n",
       "14  crowdstrike  As an example we all know Comment Crew AKA Com...   \n",
       "15  crowdstrike  This actor has many other names in the informa...   \n",
       "16  crowdstrike  In his blog Dmitri also notes that FANCY BEAR ...   \n",
       "17  crowdstrike  Although the groups exact motives remain uncle...   \n",
       "18  crowdstrike  The adversary picked in our evaluation was GOT...   \n",
       "19      fireeye  The discovery and attribution of APT1 to China...   \n",
       "20      fireeye  APT10 MenuPass Group is a Chinese cyber espion...   \n",
       "21      fireeye  In July 2018 FireEye devices detected and bloc...   \n",
       "22      fireeye  The Chinabased threat group FireEye tracks as ...   \n",
       "23      fireeye  Cyber espionage actors now designated by FireE...   \n",
       "\n",
       "                                             keywords  \n",
       "0                                     [APT17, Aurora]  \n",
       "1                  [Platinum, APT16, EvilPost, SPIVY]  \n",
       "2                            [APT16, ELMER, backdoor]  \n",
       "3                          [admin@338, Temper, Panda]  \n",
       "4                            [APT16, EvilPost, Danti]  \n",
       "5                                     [APT1, Chinese]  \n",
       "6                                   [BlackOasis, APT]  \n",
       "7   [Sofacy, APT28, Pawn, Storm, Sednit, STRONTIUM...  \n",
       "8   [Sofacy, apt28, sandworm, x-agent, pawn, storm...  \n",
       "9            [Sofacy, APT28, Fancy, Bear, Tsar, Team]  \n",
       "10           [Sofacy, APT28, Fancy, Bear, Tsar, Team]  \n",
       "11    [Sofacy, Fancy, Bear, Sednit, STRONTIUM, APT28]  \n",
       "12                              [STONE, PANDA, APT10]  \n",
       "13                              [GOTHIC, PANDA, APT3]  \n",
       "14                       [Crew, Team, APT-1, Comment]  \n",
       "15       [APT-29, Office, Monkeys, CozyCar, CozyDuke]  \n",
       "16                         [FANCY, BEAR, Sofacy, APT]  \n",
       "17                              [GOTHIC, PANDA, APT3]  \n",
       "18                              [GOTHIC, PANDA, APT3]  \n",
       "19                                   [APT1, PLA, GSD]  \n",
       "20                                  [APT10, MenuPass]  \n",
       "21                                  [APT10, Menupass]  \n",
       "22                                        [APT3, UPS]  \n",
       "23                                [APT32, OceanLotus]  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for index, row in df.iterrows():\n",
    "    sentence = row.text\n",
    "    s = set(map(lambda x: x.lower(), row.keywords))\n",
    "    text = []\n",
    "    for word in sentence.split(' '):\n",
    "        word = word.strip()\n",
    "        if word != '' and word.lower() in s:\n",
    "            label = \"GROUP_NAME\"\n",
    "        else:\n",
    "            label = \"O\"\n",
    "        text.append([word, label])\n",
    "    result.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['Analysis', 'O'],\n",
       "  ['of', 'O'],\n",
       "  ['the', 'O'],\n",
       "  ['malicious', 'O'],\n",
       "  ['CCleaner', 'O'],\n",
       "  ['code', 'O'],\n",
       "  ['allowed', 'O'],\n",
       "  ['us', 'O'],\n",
       "  ['to', 'O'],\n",
       "  ['correlate', 'O'],\n",
       "  ['it', 'O'],\n",
       "  ['with', 'O'],\n",
       "  ['a', 'O'],\n",
       "  ['couple', 'O'],\n",
       "  ['of', 'O'],\n",
       "  ['other', 'O'],\n",
       "  ['backdoors', 'O'],\n",
       "  ['that', 'O'],\n",
       "  ['are', 'O'],\n",
       "  ['known', 'O'],\n",
       "  ['to', 'O'],\n",
       "  ['have', 'O'],\n",
       "  ['been', 'O'],\n",
       "  ['used', 'O'],\n",
       "  ['in', 'O'],\n",
       "  ['the', 'O'],\n",
       "  ['past', 'O'],\n",
       "  ['by', 'O'],\n",
       "  ['APT', 'O'],\n",
       "  ['groups', 'O'],\n",
       "  ['from', 'O'],\n",
       "  ['the', 'O'],\n",
       "  ['Axiom', 'O'],\n",
       "  ['umbrella', 'O'],\n",
       "  ['such', 'O'],\n",
       "  ['as', 'O'],\n",
       "  ['APT17', 'GROUP_NAME'],\n",
       "  ['also', 'O'],\n",
       "  ['known', 'O'],\n",
       "  ['as', 'O'],\n",
       "  ['Aurora', 'GROUP_NAME']],\n",
       " [['The', 'O'],\n",
       "  ['Platinum', 'GROUP_NAME'],\n",
       "  ['APT16', 'GROUP_NAME'],\n",
       "  ['EvilPost', 'GROUP_NAME'],\n",
       "  ['and', 'O'],\n",
       "  ['SPIVY', 'GROUP_NAME'],\n",
       "  ['groups', 'O'],\n",
       "  ['were', 'O'],\n",
       "  ['already', 'O'],\n",
       "  ['known', 'O'],\n",
       "  ['to', 'O'],\n",
       "  ['use', 'O'],\n",
       "  ['this', 'O'],\n",
       "  ['exploit', 'O']],\n",
       " [['This', 'O'],\n",
       "  ['variant', 'O'],\n",
       "  ['was', 'O'],\n",
       "  ['also', 'O'],\n",
       "  ['used', 'O'],\n",
       "  ['by', 'O'],\n",
       "  ['the', 'O'],\n",
       "  ['APT16', 'GROUP_NAME'],\n",
       "  ['group', 'O'],\n",
       "  ['ELMER', 'GROUP_NAME'],\n",
       "  ['backdoor', 'GROUP_NAME'],\n",
       "  ['in', 'O'],\n",
       "  ['Taiwan', 'O'],\n",
       "  ['in', 'O'],\n",
       "  ['December', 'O'],\n",
       "  ['2015', 'O']],\n",
       " [['The', 'O'],\n",
       "  ['CnC', 'O'],\n",
       "  ['has', 'O'],\n",
       "  ['been', 'O'],\n",
       "  ['used', 'O'],\n",
       "  ['in', 'O'],\n",
       "  ['other', 'O'],\n",
       "  ['APT', 'O'],\n",
       "  ['incidents', 'O'],\n",
       "  ['attributed', 'O'],\n",
       "  ['by', 'O'],\n",
       "  ['FireEye', 'O'],\n",
       "  ['to', 'O'],\n",
       "  ['the', 'O'],\n",
       "  ['group', 'O'],\n",
       "  ['admin338', 'O'],\n",
       "  ['aka', 'O'],\n",
       "  ['Temper', 'GROUP_NAME'],\n",
       "  ['Panda', 'GROUP_NAME']],\n",
       " [['The', 'O'],\n",
       "  ['TwoForOne', 'O'],\n",
       "  ['Platinum', 'O'],\n",
       "  ['group', 'O'],\n",
       "  ['is', 'O'],\n",
       "  ['described', 'O'],\n",
       "  ['in', 'O'],\n",
       "  ['Microsoft', 'O'],\n",
       "  ['research', 'O'],\n",
       "  ['APT16', 'GROUP_NAME'],\n",
       "  ['in', 'O'],\n",
       "  ['FireEye', 'O'],\n",
       "  ['reports', 'O'],\n",
       "  ['and', 'O'],\n",
       "  ['EvilPost', 'GROUP_NAME'],\n",
       "  ['and', 'O'],\n",
       "  ['Danti', 'GROUP_NAME'],\n",
       "  ['in', 'O'],\n",
       "  ['Kaspersky', 'O'],\n",
       "  ['Lab', 'O'],\n",
       "  ['private', 'O'],\n",
       "  ['reports', 'O']],\n",
       " [['The', 'O'],\n",
       "  ['Mandiant', 'O'],\n",
       "  ['reports', 'O'],\n",
       "  ['starts', 'O'],\n",
       "  ['off', 'O'],\n",
       "  ['by', 'O'],\n",
       "  ['stating', 'O'],\n",
       "  ['that', 'O'],\n",
       "  ['APT1', 'GROUP_NAME'],\n",
       "  ['appears', 'O'],\n",
       "  ['to', 'O'],\n",
       "  ['be', 'O'],\n",
       "  ['a', 'O'],\n",
       "  ['division', 'O'],\n",
       "  ['of', 'O'],\n",
       "  ['the', 'O'],\n",
       "  ['Chinese', 'GROUP_NAME'],\n",
       "  ['army', 'O']],\n",
       " [['BlackOasis', 'GROUP_NAME'],\n",
       "  ['is', 'O'],\n",
       "  ['an', 'O'],\n",
       "  ['APT', 'GROUP_NAME'],\n",
       "  ['group', 'O'],\n",
       "  ['we', 'O'],\n",
       "  ['have', 'O'],\n",
       "  ['been', 'O'],\n",
       "  ['tracking', 'O'],\n",
       "  ['since', 'O'],\n",
       "  ['May', 'O'],\n",
       "  ['2016', 'O']],\n",
       " [['In', 'O'],\n",
       "  ['its', 'O'],\n",
       "  ['affidavit', 'O'],\n",
       "  ['for', 'O'],\n",
       "  ['sinkholing', 'O'],\n",
       "  ['the', 'O'],\n",
       "  ['C2', 'O'],\n",
       "  ['the', 'O'],\n",
       "  ['FBI', 'O'],\n",
       "  ['suggests', 'O'],\n",
       "  ['that', 'O'],\n",
       "  ['Sofacy', 'GROUP_NAME'],\n",
       "  ['aka', 'O'],\n",
       "  ['APT28', 'GROUP_NAME'],\n",
       "  ['Pawn', 'GROUP_NAME'],\n",
       "  ['Storm', 'GROUP_NAME'],\n",
       "  ['Sednit', 'GROUP_NAME'],\n",
       "  ['STRONTIUM', 'GROUP_NAME'],\n",
       "  ['and', 'O'],\n",
       "  ['Tsar', 'GROUP_NAME'],\n",
       "  ['Team', 'GROUP_NAME'],\n",
       "  ['is', 'O'],\n",
       "  ['the', 'O'],\n",
       "  ['culprit', 'O']],\n",
       " [['Sofacy', 'GROUP_NAME'],\n",
       "  ['Group', 'O'],\n",
       "  ['also', 'O'],\n",
       "  ['known', 'O'],\n",
       "  ['as', 'O'],\n",
       "  ['apt28', 'GROUP_NAME'],\n",
       "  ['sandworm', 'GROUP_NAME'],\n",
       "  ['xagent', 'O'],\n",
       "  ['pawn', 'GROUP_NAME'],\n",
       "  ['storm', 'GROUP_NAME'],\n",
       "  ['fancy', 'GROUP_NAME'],\n",
       "  ['bear', 'GROUP_NAME'],\n",
       "  ['and', 'O'],\n",
       "  ['sednit', 'GROUP_NAME']],\n",
       " [['Sofacy', 'GROUP_NAME'],\n",
       "  ['aka', 'O'],\n",
       "  ['APT28', 'GROUP_NAME'],\n",
       "  ['Fancy', 'GROUP_NAME'],\n",
       "  ['Bear', 'GROUP_NAME'],\n",
       "  ['and', 'O'],\n",
       "  ['Tsar', 'GROUP_NAME'],\n",
       "  ['Team', 'GROUP_NAME'],\n",
       "  ['is', 'O'],\n",
       "  ['a', 'O'],\n",
       "  ['highly', 'O'],\n",
       "  ['active', 'O'],\n",
       "  ['and', 'O'],\n",
       "  ['prolific', 'O'],\n",
       "  ['cyberespionage', 'O'],\n",
       "  ['group', 'O'],\n",
       "  ['that', 'O'],\n",
       "  ['Kaspersky', 'O'],\n",
       "  ['Lab', 'O'],\n",
       "  ['has', 'O'],\n",
       "  ['been', 'O'],\n",
       "  ['tracking', 'O'],\n",
       "  ['for', 'O'],\n",
       "  ['many', 'O'],\n",
       "  ['years', 'O']],\n",
       " [['Sofacy', 'GROUP_NAME'],\n",
       "  ['also', 'O'],\n",
       "  ['known', 'O'],\n",
       "  ['as', 'O'],\n",
       "  ['APT28', 'GROUP_NAME'],\n",
       "  ['Fancy', 'GROUP_NAME'],\n",
       "  ['Bear', 'GROUP_NAME'],\n",
       "  ['and', 'O'],\n",
       "  ['Tsar', 'GROUP_NAME'],\n",
       "  ['Team', 'GROUP_NAME'],\n",
       "  ['is', 'O'],\n",
       "  ['a', 'O'],\n",
       "  ['highly', 'O'],\n",
       "  ['active', 'O'],\n",
       "  ['and', 'O'],\n",
       "  ['prolific', 'O'],\n",
       "  ['APT', 'O']],\n",
       " [['Sofacy', 'GROUP_NAME'],\n",
       "  ['also', 'O'],\n",
       "  ['known', 'O'],\n",
       "  ['as', 'O'],\n",
       "  ['Fancy', 'GROUP_NAME'],\n",
       "  ['Bear', 'GROUP_NAME'],\n",
       "  ['Sednit', 'GROUP_NAME'],\n",
       "  ['STRONTIUM', 'GROUP_NAME'],\n",
       "  ['and', 'O'],\n",
       "  ['APT28', 'GROUP_NAME'],\n",
       "  ['is', 'O'],\n",
       "  ['an', 'O'],\n",
       "  ['advanced', 'O'],\n",
       "  ['threat', 'O'],\n",
       "  ['group', 'O'],\n",
       "  ['that', 'O'],\n",
       "  ['has', 'O'],\n",
       "  ['been', 'O'],\n",
       "  ['active', 'O'],\n",
       "  ['since', 'O'],\n",
       "  ['around', 'O'],\n",
       "  ['2008', 'O'],\n",
       "  ['targeting', 'O'],\n",
       "  ['mostly', 'O'],\n",
       "  ['military', 'O'],\n",
       "  ['and', 'O'],\n",
       "  ['government', 'O'],\n",
       "  ['entities', 'O'],\n",
       "  ['worldwide', 'O'],\n",
       "  ['with', 'O'],\n",
       "  ['a', 'O'],\n",
       "  ['focus', 'O'],\n",
       "  ['on', 'O'],\n",
       "  ['NATO', 'O'],\n",
       "  ['countries', 'O']],\n",
       " [['Recently', 'O'],\n",
       "  ['in', 'O'],\n",
       "  ['July', 'O'],\n",
       "  ['and', 'O'],\n",
       "  ['August', 'O'],\n",
       "  ['2018', 'O'],\n",
       "  ['IntrusionTruth', 'O'],\n",
       "  ['has', 'O'],\n",
       "  ['returned', 'O'],\n",
       "  ['with', 'O'],\n",
       "  ['new', 'O'],\n",
       "  ['reporting', 'O'],\n",
       "  ['regarding', 'O'],\n",
       "  ['actors', 'O'],\n",
       "  ['with', 'O'],\n",
       "  ['ties', 'O'],\n",
       "  ['to', 'O'],\n",
       "  ['historic', 'O'],\n",
       "  ['STONE', 'GROUP_NAME'],\n",
       "  ['PANDA', 'GROUP_NAME'],\n",
       "  ['APT10', 'GROUP_NAME'],\n",
       "  ['activity', 'O'],\n",
       "  ['and', 'O'],\n",
       "  ['has', 'O'],\n",
       "  ['ultimately', 'O'],\n",
       "  ['associated', 'O'],\n",
       "  ['them', 'O'],\n",
       "  ['with', 'O'],\n",
       "  ['the', 'O'],\n",
       "  ['MSS', 'O'],\n",
       "  ['Tianjin', 'O'],\n",
       "  ['Bureau', 'O'],\n",
       "  ['Though', 'O'],\n",
       "  ['CrowdStrike', 'O'],\n",
       "  ['Falcon', 'O'],\n",
       "  ['Intelligence', 'O'],\n",
       "  ['is', 'O'],\n",
       "  ['currently', 'O'],\n",
       "  ['unable', 'O'],\n",
       "  ['to', 'O'],\n",
       "  ['confirm', 'O'],\n",
       "  ['all', 'O'],\n",
       "  ['of', 'O'],\n",
       "  ['the', 'O'],\n",
       "  ['details', 'O'],\n",
       "  ['provided', 'O'],\n",
       "  ['in', 'O'],\n",
       "  ['these', 'O'],\n",
       "  ['most', 'O'],\n",
       "  ['recent', 'O'],\n",
       "  ['posts', 'O'],\n",
       "  ['with', 'O'],\n",
       "  ['a', 'O'],\n",
       "  ['high', 'O'],\n",
       "  ['degree', 'O'],\n",
       "  ['of', 'O'],\n",
       "  ['confidence', 'O'],\n",
       "  ['several', 'O'],\n",
       "  ['key', 'O'],\n",
       "  ['pieces', 'O'],\n",
       "  ['of', 'O'],\n",
       "  ['information', 'O'],\n",
       "  ['can', 'O'],\n",
       "  ['be', 'O'],\n",
       "  ['verified', 'O']],\n",
       " [['Although', 'O'],\n",
       "  ['the', 'O'],\n",
       "  ['groups', 'O'],\n",
       "  ['exact', 'O'],\n",
       "  ['motives', 'O'],\n",
       "  ['remain', 'O'],\n",
       "  ['unclear', 'O'],\n",
       "  ['its', 'O'],\n",
       "  ['initial', 'O'],\n",
       "  ['tranche', 'O'],\n",
       "  ['of', 'O'],\n",
       "  ['information', 'O'],\n",
       "  ['exposed', 'O'],\n",
       "  ['individuals', 'O'],\n",
       "  ['connected', 'O'],\n",
       "  ['to', 'O'],\n",
       "  ['longrunning', 'O'],\n",
       "  ['GOTHIC', 'GROUP_NAME'],\n",
       "  ['PANDA', 'GROUP_NAME'],\n",
       "  ['APT3', 'GROUP_NAME'],\n",
       "  ['operations', 'O'],\n",
       "  ['culminating', 'O'],\n",
       "  ['in', 'O'],\n",
       "  ['a', 'O'],\n",
       "  ['connection', 'O'],\n",
       "  ['to', 'O'],\n",
       "  ['the', 'O'],\n",
       "  ['Chinese', 'O'],\n",
       "  ['firm', 'O'],\n",
       "  ['Boyusec', 'O'],\n",
       "  ['and', 'O'],\n",
       "  ['ultimately', 'O'],\n",
       "  ['Chinese', 'O'],\n",
       "  ['Ministry', 'O'],\n",
       "  ['of', 'O'],\n",
       "  ['State', 'O'],\n",
       "  ['Security', 'O'],\n",
       "  ['MSS', 'O'],\n",
       "  ['entities', 'O'],\n",
       "  ['in', 'O'],\n",
       "  ['Guangzhou', 'O']],\n",
       " [['As', 'O'],\n",
       "  ['an', 'O'],\n",
       "  ['example', 'O'],\n",
       "  ['we', 'O'],\n",
       "  ['all', 'O'],\n",
       "  ['know', 'O'],\n",
       "  ['Comment', 'GROUP_NAME'],\n",
       "  ['Crew', 'GROUP_NAME'],\n",
       "  ['AKA', 'O'],\n",
       "  ['Comment', 'GROUP_NAME'],\n",
       "  ['Team', 'GROUP_NAME'],\n",
       "  ['AKA', 'O'],\n",
       "  ['APT1', 'O'],\n",
       "  ['AKA', 'O'],\n",
       "  ['Comment', 'GROUP_NAME']],\n",
       " [['This', 'O'],\n",
       "  ['actor', 'O'],\n",
       "  ['has', 'O'],\n",
       "  ['many', 'O'],\n",
       "  ['other', 'O'],\n",
       "  ['names', 'O'],\n",
       "  ['in', 'O'],\n",
       "  ['the', 'O'],\n",
       "  ['information', 'O'],\n",
       "  ['security', 'O'],\n",
       "  ['community', 'O'],\n",
       "  ['including', 'O'],\n",
       "  ['APT29', 'O'],\n",
       "  ['Office', 'GROUP_NAME'],\n",
       "  ['Monkeys', 'GROUP_NAME'],\n",
       "  ['CozyCar', 'GROUP_NAME'],\n",
       "  ['and', 'O'],\n",
       "  ['CozyDuke', 'GROUP_NAME']],\n",
       " [['In', 'O'],\n",
       "  ['his', 'O'],\n",
       "  ['blog', 'O'],\n",
       "  ['Dmitri', 'O'],\n",
       "  ['also', 'O'],\n",
       "  ['notes', 'O'],\n",
       "  ['that', 'O'],\n",
       "  ['FANCY', 'GROUP_NAME'],\n",
       "  ['BEAR', 'GROUP_NAME'],\n",
       "  ['also', 'O'],\n",
       "  ['known', 'O'],\n",
       "  ['as', 'O'],\n",
       "  ['Sofacy', 'GROUP_NAME'],\n",
       "  ['or', 'O'],\n",
       "  ['APT', 'GROUP_NAME'],\n",
       "  ['28', 'O'],\n",
       "  ['is', 'O'],\n",
       "  ['a', 'O'],\n",
       "  ['Russianbased', 'O'],\n",
       "  ['threat', 'O'],\n",
       "  ['actor', 'O'],\n",
       "  ['whose', 'O'],\n",
       "  ['attacks', 'O'],\n",
       "  ['have', 'O'],\n",
       "  ['ranged', 'O'],\n",
       "  ['far', 'O'],\n",
       "  ['beyond', 'O'],\n",
       "  ['the', 'O'],\n",
       "  ['United', 'O'],\n",
       "  ['States', 'O'],\n",
       "  ['and', 'O'],\n",
       "  ['Western', 'O'],\n",
       "  ['Europe', 'O']],\n",
       " [['Although', 'O'],\n",
       "  ['the', 'O'],\n",
       "  ['groups', 'O'],\n",
       "  ['exact', 'O'],\n",
       "  ['motives', 'O'],\n",
       "  ['remain', 'O'],\n",
       "  ['unclear', 'O'],\n",
       "  ['its', 'O'],\n",
       "  ['initial', 'O'],\n",
       "  ['tranche', 'O'],\n",
       "  ['of', 'O'],\n",
       "  ['information', 'O'],\n",
       "  ['exposed', 'O'],\n",
       "  ['individuals', 'O'],\n",
       "  ['connected', 'O'],\n",
       "  ['to', 'O'],\n",
       "  ['longrunning', 'O'],\n",
       "  ['GOTHIC', 'GROUP_NAME'],\n",
       "  ['PANDA', 'GROUP_NAME'],\n",
       "  ['APT3', 'GROUP_NAME'],\n",
       "  ['operations', 'O']],\n",
       " [['The', 'O'],\n",
       "  ['adversary', 'O'],\n",
       "  ['picked', 'O'],\n",
       "  ['in', 'O'],\n",
       "  ['our', 'O'],\n",
       "  ['evaluation', 'O'],\n",
       "  ['was', 'O'],\n",
       "  ['GOTHIC', 'GROUP_NAME'],\n",
       "  ['PANDA', 'GROUP_NAME'],\n",
       "  ['also', 'O'],\n",
       "  ['known', 'O'],\n",
       "  ['as', 'O'],\n",
       "  ['APT3', 'GROUP_NAME'],\n",
       "  ['a', 'O'],\n",
       "  ['sophisticated', 'O'],\n",
       "  ['hacking', 'O'],\n",
       "  ['team', 'O'],\n",
       "  ['linked', 'O'],\n",
       "  ['to', 'O'],\n",
       "  ['the', 'O'],\n",
       "  ['Chinese', 'O'],\n",
       "  ['governments', 'O'],\n",
       "  ['Ministry', 'O'],\n",
       "  ['of', 'O'],\n",
       "  ['State', 'O'],\n",
       "  ['Security', 'O'],\n",
       "  ['MSS', 'O']],\n",
       " [['The', 'O'],\n",
       "  ['discovery', 'O'],\n",
       "  ['and', 'O'],\n",
       "  ['attribution', 'O'],\n",
       "  ['of', 'O'],\n",
       "  ['APT1', 'GROUP_NAME'],\n",
       "  ['to', 'O'],\n",
       "  ['Chinas', 'O'],\n",
       "  ['2nd', 'O'],\n",
       "  ['Bureau', 'O'],\n",
       "  ['of', 'O'],\n",
       "  ['the', 'O'],\n",
       "  ['Peoples', 'O'],\n",
       "  ['Liberation', 'O'],\n",
       "  ['Army', 'O'],\n",
       "  ['PLA', 'GROUP_NAME'],\n",
       "  ['General', 'O'],\n",
       "  ['Staff', 'O'],\n",
       "  ['Departments', 'O'],\n",
       "  ['GSD', 'GROUP_NAME'],\n",
       "  ['3rd', 'O'],\n",
       "  ['Department', 'O'],\n",
       "  ['Military', 'O'],\n",
       "  ['Unit', 'O'],\n",
       "  ['Cover', 'O'],\n",
       "  ['Designator', 'O'],\n",
       "  ['61398', 'O'],\n",
       "  ['also', 'O'],\n",
       "  ['elevated', 'O'],\n",
       "  ['the', 'O'],\n",
       "  ['public', 'O'],\n",
       "  ['dialogue', 'O'],\n",
       "  ['about', 'O'],\n",
       "  ['cyber', 'O'],\n",
       "  ['espionage', 'O'],\n",
       "  ['and', 'O'],\n",
       "  ['the', 'O'],\n",
       "  ['theft', 'O'],\n",
       "  ['of', 'O'],\n",
       "  ['intellectual', 'O'],\n",
       "  ['property', 'O'],\n",
       "  ['to', 'O'],\n",
       "  ['a', 'O'],\n",
       "  ['level', 'O'],\n",
       "  ['not', 'O'],\n",
       "  ['seen', 'O'],\n",
       "  ['before', 'O']],\n",
       " [['APT10', 'GROUP_NAME'],\n",
       "  ['MenuPass', 'GROUP_NAME'],\n",
       "  ['Group', 'O'],\n",
       "  ['is', 'O'],\n",
       "  ['a', 'O'],\n",
       "  ['Chinese', 'O'],\n",
       "  ['cyber', 'O'],\n",
       "  ['espionage', 'O'],\n",
       "  ['group', 'O'],\n",
       "  ['that', 'O'],\n",
       "  ['FireEye', 'O'],\n",
       "  ['has', 'O'],\n",
       "  ['tracked', 'O'],\n",
       "  ['since', 'O'],\n",
       "  ['2009', 'O']],\n",
       " [['In', 'O'],\n",
       "  ['July', 'O'],\n",
       "  ['2018', 'O'],\n",
       "  ['FireEye', 'O'],\n",
       "  ['devices', 'O'],\n",
       "  ['detected', 'O'],\n",
       "  ['and', 'O'],\n",
       "  ['blocked', 'O'],\n",
       "  ['what', 'O'],\n",
       "  ['appears', 'O'],\n",
       "  ['to', 'O'],\n",
       "  ['be', 'O'],\n",
       "  ['APT10', 'GROUP_NAME'],\n",
       "  ['Menupass', 'GROUP_NAME'],\n",
       "  ['activity', 'O'],\n",
       "  ['targeting', 'O'],\n",
       "  ['the', 'O'],\n",
       "  ['Japanese', 'O'],\n",
       "  ['media', 'O'],\n",
       "  ['sector', 'O']],\n",
       " [['The', 'O'],\n",
       "  ['Chinabased', 'O'],\n",
       "  ['threat', 'O'],\n",
       "  ['group', 'O'],\n",
       "  ['FireEye', 'O'],\n",
       "  ['tracks', 'O'],\n",
       "  ['as', 'O'],\n",
       "  ['APT3', 'GROUP_NAME'],\n",
       "  ['aka', 'O'],\n",
       "  ['UPS', 'GROUP_NAME']],\n",
       " [['Cyber', 'O'],\n",
       "  ['espionage', 'O'],\n",
       "  ['actors', 'O'],\n",
       "  ['now', 'O'],\n",
       "  ['designated', 'O'],\n",
       "  ['by', 'O'],\n",
       "  ['FireEye', 'O'],\n",
       "  ['as', 'O'],\n",
       "  ['APT32', 'GROUP_NAME'],\n",
       "  ['OceanLotus', 'GROUP_NAME'],\n",
       "  ['Group', 'O']]]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addCharInformatioin(Sentences):\n",
    "    for i,sentence in enumerate(Sentences):\n",
    "        for j,data in enumerate(sentence):\n",
    "            chars = [c for c in data[0]]\n",
    "            Sentences[i][j] = [data[0],chars,data[1]]\n",
    "    return Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = addCharInformatioin(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSentences = result[:int(len(result) * .8)]\n",
    "testSentences = result[int(len(result) * .8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelSet = set()\n",
    "words = {}\n",
    "\n",
    "for dataset in [trainSentences, testSentences]:\n",
    "    for sentence in dataset:\n",
    "        for token,char,label in sentence:\n",
    "            labelSet.add(label)\n",
    "            words[token.lower()] = True\n",
    "\n",
    "label2Idx = {}\n",
    "for label in labelSet:\n",
    "    label2Idx[label] = len(label2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "case2Idx = {'numeric': 0, 'allLower':1, 'allUpper':2, 'initialUpper':3, 'other':4, 'mainly_numeric':5, 'contains_digit': 6, 'PADDING_TOKEN':7}\n",
    "caseEmbeddings = np.identity(len(case2Idx), dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2Idx = {}\n",
    "wordEmbeddings = []\n",
    "\n",
    "# fEmbeddings = open(\"glove.6B.100d.txt\")\n",
    "fEmbeddings = open(\"word2vec.txt\")\n",
    "\n",
    "for line in fEmbeddings:\n",
    "    split = line.strip().split(\" \")\n",
    "    word = split[0]\n",
    "    \n",
    "    if len(word2Idx) == 0: #Add padding+unknown\n",
    "        word2Idx[\"PADDING_TOKEN\"] = len(word2Idx)\n",
    "        vector = np.zeros(len(split)-1) #Zero vector vor 'PADDING' word\n",
    "        wordEmbeddings.append(vector)\n",
    "        \n",
    "        word2Idx[\"UNKNOWN_TOKEN\"] = len(word2Idx)\n",
    "        vector = np.random.uniform(-0.25, 0.25, len(split)-1)\n",
    "        wordEmbeddings.append(vector)\n",
    "\n",
    "    if split[0].lower() in words:\n",
    "        vector = np.array([float(num) for num in split[1:]])\n",
    "        wordEmbeddings.append(vector)\n",
    "        word2Idx[split[0]] = len(word2Idx)\n",
    "        \n",
    "wordEmbeddings = np.array(wordEmbeddings)\n",
    "\n",
    "char2Idx = {\"PADDING\":0, \"UNKNOWN\":1}\n",
    "for c in \" 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ.,-_()[]{}!?:;#'\\\"/\\\\%$`&=*+@^~|\":\n",
    "    char2Idx[c] = len(char2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.04564428,  0.21211099, -0.06987944, ..., -0.24225977,\n",
       "        -0.24161206, -0.01009185],\n",
       "       [ 0.26070642, -1.1021085 , -0.2649278 , ..., -0.00195244,\n",
       "         0.97385913, -0.30661538],\n",
       "       ...,\n",
       "       [ 0.03101471, -0.06764489, -0.02508294, ..., -0.01494516,\n",
       "         0.0385602 , -0.01278899],\n",
       "       [ 0.0341464 , -0.07367665, -0.02550976, ..., -0.01468766,\n",
       "         0.04396356, -0.01185723],\n",
       "       [ 0.02275341, -0.04528004, -0.01716731, ..., -0.01916434,\n",
       "         0.01984791, -0.01104459]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createMatrices(sentences, word2Idx, label2Idx, case2Idx,char2Idx):\n",
    "    unknownIdx = word2Idx['UNKNOWN_TOKEN']\n",
    "    paddingIdx = word2Idx['PADDING_TOKEN']    \n",
    "        \n",
    "    dataset = []\n",
    "    \n",
    "    wordCount = 0\n",
    "    unknownWordCount = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        wordIndices = []    \n",
    "        caseIndices = []\n",
    "        charIndices = []\n",
    "        labelIndices = []\n",
    "        \n",
    "        for word,char,label in sentence:  \n",
    "            wordCount += 1\n",
    "            if word in word2Idx:\n",
    "                wordIdx = word2Idx[word]\n",
    "            elif word.lower() in word2Idx:\n",
    "                wordIdx = word2Idx[word.lower()]                 \n",
    "            else:\n",
    "                wordIdx = unknownIdx\n",
    "                unknownWordCount += 1\n",
    "            charIdx = []\n",
    "            for x in char:\n",
    "                charIdx.append(char2Idx[x])\n",
    "            #Get the label and map to int            \n",
    "            wordIndices.append(wordIdx)\n",
    "            caseIndices.append(getCasing(word, case2Idx))\n",
    "            charIndices.append(charIdx)\n",
    "            labelIndices.append(label2Idx[label])\n",
    "        dataset.append([wordIndices, caseIndices, charIndices, labelIndices]) \n",
    "        \n",
    "    return dataset\n",
    "\n",
    "def padding(Sentences):\n",
    "    maxlen = 52\n",
    "    for sentence in Sentences:\n",
    "        char = sentence[2]\n",
    "        for x in char:\n",
    "            maxlen = max(maxlen,len(x))\n",
    "    for i,sentence in enumerate(Sentences):\n",
    "        Sentences[i][2] = pad_sequences(Sentences[i][2],52,padding='post')\n",
    "    return Sentences\n",
    "\n",
    "def getCasing(word, caseLookup):   \n",
    "    casing = 'other'\n",
    "    \n",
    "    numDigits = 0\n",
    "    for char in word:\n",
    "        if char.isdigit():\n",
    "            numDigits += 1\n",
    "            \n",
    "    digitFraction = numDigits / float(len(word))\n",
    "    \n",
    "    if word.isdigit():\n",
    "        casing = 'numeric'\n",
    "    elif digitFraction > 0.5:\n",
    "        casing = 'mainly_numeric'\n",
    "    elif word.islower():\n",
    "        casing = 'allLower'\n",
    "    elif word.isupper():\n",
    "        casing = 'allUpper'\n",
    "    elif word[0].isupper():\n",
    "        casing = 'initialUpper'\n",
    "    elif numDigits > 0:\n",
    "        casing = 'contains_digit'\n",
    "    return caseLookup[casing]\n",
    "\n",
    "train_set = padding(createMatrices(trainSentences,word2Idx,  label2Idx, case2Idx,char2Idx))\n",
    "test_set = padding(createMatrices(testSentences, word2Idx, label2Idx, case2Idx,char2Idx))\n",
    "\n",
    "idx2Label = {v: k for k, v in label2Idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_set[0][3]\n",
    "# char2Idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createBatches(data):\n",
    "    l = []\n",
    "    for i in data:\n",
    "        l.append(len(i[0]))\n",
    "    l = set(l)\n",
    "    batches = []\n",
    "    batch_len = []\n",
    "    z = 0\n",
    "    for i in l:\n",
    "        for batch in data:\n",
    "            if len(batch[0]) == i:\n",
    "                batches.append(batch)\n",
    "                z += 1\n",
    "        batch_len.append(z)\n",
    "    return batches,batch_len\n",
    "\n",
    "train_batch,train_batch_len = createBatches(train_set)\n",
    "test_batch,test_batch_len = createBatches(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "char_input (InputLayer)         (None, None, 52)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "char_embedding (TimeDistributed (None, None, 52, 30) 2850        char_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, None, 52, 30) 0           char_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_29 (TimeDistri (None, None, 52, 30) 2730        dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_30 (TimeDistri (None, None, 1, 30)  0           time_distributed_29[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "words_input (InputLayer)        (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "casing_input (InputLayer)       (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_31 (TimeDistri (None, None, 30)     0           time_distributed_30[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "embedding_22 (Embedding)        (None, None, 100)    22000       words_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_23 (Embedding)        (None, None, 8)      64          casing_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, None, 30)     0           time_distributed_31[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, None, 138)    0           embedding_22[0][0]               \n",
      "                                                                 embedding_23[0][0]               \n",
      "                                                                 dropout_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_8 (Bidirectional) (None, None, 40)     25440       concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_32 (TimeDistri (None, None, 2)      82          bidirectional_8[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 53,166\n",
      "Trainable params: 31,102\n",
      "Non-trainable params: 22,064\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "words_input = Input(shape=(None,),dtype='int32',name='words_input')\n",
    "\n",
    "words = Embedding(input_dim=wordEmbeddings.shape[0], output_dim=100,  weights=[wordEmbeddings], trainable=False)(words_input)\n",
    "\n",
    "casing_input = Input(shape=(None,), dtype='int32', name='casing_input')\n",
    "casing = Embedding(output_dim=caseEmbeddings.shape[1], input_dim=caseEmbeddings.shape[0], weights=[caseEmbeddings], trainable=False)(casing_input)\n",
    "\n",
    "character_input=Input(shape=(None,52,),name='char_input')\n",
    "embed_char_out=TimeDistributed(Embedding(len(char2Idx),30,embeddings_initializer=RandomUniform(minval=-0.5, maxval=0.5)), name='char_embedding')(character_input)\n",
    "\n",
    "dropout= Dropout(0.5)(embed_char_out)\n",
    "\n",
    "conv1d_out= TimeDistributed(Conv1D(kernel_size=3, filters=30, padding='same',activation='tanh', strides=1))(dropout)\n",
    "\n",
    "maxpool_out=TimeDistributed(MaxPooling1D(52))(conv1d_out)\n",
    "\n",
    "char = TimeDistributed(Flatten())(maxpool_out)\n",
    "char = Dropout(0.5)(char)\n",
    "\n",
    "output = concatenate([words, casing,char])\n",
    "output = Bidirectional(LSTM(20, return_sequences=True, dropout=0.50, recurrent_dropout=0.25))(output)\n",
    "output = TimeDistributed(Dense(len(label2Idx), activation='softmax'))(output)\n",
    "\n",
    "model = Model(inputs=[words_input, casing_input,character_input], outputs=[output])\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='nadam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/50\n",
      "13/16 [=======================>......] - ETA: 1s \n",
      "Epoch 1/50\n",
      "15/16 [===========================>..] - ETA: 0s \n",
      "Epoch 2/50\n",
      "13/16 [=======================>......] - ETA: 0s \n",
      "Epoch 3/50\n",
      "15/16 [===========================>..] - ETA: 0s \n",
      "Epoch 4/50\n",
      "13/16 [=======================>......] - ETA: 0s \n",
      "Epoch 5/50\n",
      "15/16 [===========================>..] - ETA: 0s \n",
      "Epoch 6/50\n",
      "13/16 [=======================>......] - ETA: 0s \n",
      "Epoch 7/50\n",
      "15/16 [===========================>..] - ETA: 0s \n",
      "Epoch 8/50\n",
      "15/16 [===========================>..] - ETA: 0s \n",
      "Epoch 9/50\n",
      "13/16 [=======================>......] - ETA: 0s \n",
      "Epoch 10/50\n",
      "13/16 [=======================>......] - ETA: 0s \n",
      "Epoch 11/50\n",
      "15/16 [===========================>..] - ETA: 0s \n",
      "Epoch 12/50\n",
      "14/16 [=========================>....] - ETA: 0s \n",
      "Epoch 13/50\n",
      "15/16 [===========================>..] - ETA: 0s \n",
      "Epoch 14/50\n",
      "15/16 [===========================>..] - ETA: 0s \n",
      "Epoch 15/50\n",
      "13/16 [=======================>......] - ETA: 0s \n",
      "Epoch 16/50\n",
      "15/16 [===========================>..] - ETA: 0s \n",
      "Epoch 17/50\n",
      "15/16 [===========================>..] - ETA: 0s \n",
      "Epoch 18/50\n",
      "15/16 [===========================>..] - ETA: 0s \n",
      "Epoch 19/50\n",
      "15/16 [===========================>..] - ETA: 0s \n",
      "Epoch 20/50\n",
      "15/16 [===========================>..] - ETA: 0s \n",
      "Epoch 21/50\n",
      "15/16 [===========================>..] - ETA: 0s \n",
      "Epoch 22/50\n",
      "15/16 [===========================>..] - ETA: 0s \n",
      "Epoch 23/50\n",
      "14/16 [=========================>....] - ETA: 0s \n",
      "Epoch 24/50\n",
      "13/16 [=======================>......] - ETA: 0s \n",
      "Epoch 25/50\n",
      "14/16 [=========================>....] - ETA: 0s \n",
      "Epoch 26/50\n",
      "15/16 [===========================>..] - ETA: 0s \n",
      "Epoch 27/50\n",
      "15/16 [===========================>..] - ETA: 0s \n",
      "Epoch 28/50\n",
      "14/16 [=========================>....] - ETA: 0s \n",
      "Epoch 29/50\n",
      "15/16 [===========================>..] - ETA: 0s \n",
      "Epoch 30/50\n",
      "14/16 [=========================>....] - ETA: 0s \n",
      "Epoch 31/50\n",
      "15/16 [===========================>..] - ETA: 0s \n",
      "Epoch 32/50\n",
      "14/16 [=========================>....] - ETA: 0s \n",
      "Epoch 33/50\n",
      "14/16 [=========================>....] - ETA: 0s \n",
      "Epoch 34/50\n",
      "14/16 [=========================>....] - ETA: 0s \n",
      "Epoch 35/50\n",
      "14/16 [=========================>....] - ETA: 0s \n",
      "Epoch 36/50\n",
      "15/16 [===========================>..] - ETA: 0s \n",
      "Epoch 37/50\n",
      "14/16 [=========================>....] - ETA: 0s \n",
      "Epoch 38/50\n",
      "15/16 [===========================>..] - ETA: 0s \n",
      "Epoch 39/50\n",
      "14/16 [=========================>....] - ETA: 0s \n",
      "Epoch 40/50\n",
      "15/16 [===========================>..] - ETA: 0s \n",
      "Epoch 41/50\n",
      "15/16 [===========================>..] - ETA: 0s \n",
      "Epoch 42/50\n",
      "15/16 [===========================>..] - ETA: 0s \n",
      "Epoch 43/50\n",
      "14/16 [=========================>....] - ETA: 0s \n",
      "Epoch 44/50\n",
      "14/16 [=========================>....] - ETA: 0s \n",
      "Epoch 45/50\n",
      "14/16 [=========================>....] - ETA: 0s \n",
      "Epoch 46/50\n",
      "14/16 [=========================>....] - ETA: 0s \n",
      "Epoch 47/50\n",
      "15/16 [===========================>..] - ETA: 0s \n",
      "Epoch 48/50\n",
      "14/16 [=========================>....] - ETA: 0s \n",
      "Epoch 49/50\n",
      "14/16 [=========================>....] - ETA: 0s \n"
     ]
    }
   ],
   "source": [
    "def iterate_minibatches(dataset,batch_len): \n",
    "    start = 0\n",
    "    for i in batch_len:\n",
    "        tokens = []\n",
    "        caseing = []\n",
    "        char = []\n",
    "        labels = []\n",
    "        data = dataset[start:i]\n",
    "        start = i\n",
    "        for dt in data:\n",
    "            t,c,ch,l = dt\n",
    "            l = np.expand_dims(l,-1)\n",
    "            tokens.append(t)\n",
    "            caseing.append(c)\n",
    "            char.append(ch)\n",
    "            labels.append(l)\n",
    "        yield np.asarray(labels),np.asarray(tokens),np.asarray(caseing),np.asarray(char)\n",
    "\n",
    "for epoch in range(epochs):    \n",
    "    print(\"Epoch %d/%d\"%(epoch,epochs))\n",
    "    a = Progbar(len(train_batch_len))\n",
    "    for i,batch in enumerate(iterate_minibatches(train_batch,train_batch_len)):\n",
    "        labels, tokens, casing,char = batch       \n",
    "        model.train_on_batch([tokens, casing,char], labels)\n",
    "        a.update(i)\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "0/5 [..............................] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "def compute_f1(predictions, correct, idx2Label): \n",
    "    label_pred = []    \n",
    "    for sentence in predictions:\n",
    "        label_pred.append([idx2Label[element] for element in sentence])\n",
    "        \n",
    "    label_correct = []    \n",
    "    for sentence in correct:\n",
    "        label_correct.append([idx2Label[element] for element in sentence])\n",
    "            \n",
    "    \n",
    "    prec = compute_precision(label_pred, label_correct)\n",
    "    rec = compute_precision(label_correct, label_pred)\n",
    "    \n",
    "    \n",
    "    f1 = 0\n",
    "    if (rec+prec) > 0:\n",
    "        f1 = 2.0 * prec * rec / (prec + rec);\n",
    "        \n",
    "    return prec, rec, f1\n",
    "\n",
    "def compute_precision(guessed_sentences, correct_sentences):\n",
    "    assert(len(guessed_sentences) == len(correct_sentences))\n",
    "    correctCount = 0\n",
    "    count = 0\n",
    "    \n",
    "    for sentenceIdx in range(len(guessed_sentences)):\n",
    "        guessed = guessed_sentences[sentenceIdx]\n",
    "        correct = correct_sentences[sentenceIdx]\n",
    "        idx = 0\n",
    "        while idx < len(guessed):    \n",
    "            count += 1\n",
    "            if guessed[idx] == correct[idx]:\n",
    "                idx += 1\n",
    "                correctlyFound = True\n",
    "                correctCount += 1\n",
    "                while idx < len(guessed) and guessed[idx][0] == 'I': #Scan until it no longer starts with I\n",
    "                    if guessed[idx] != correct[idx]:\n",
    "                        correctlyFound = False\n",
    "\n",
    "                    idx += 1\n",
    "            else:\n",
    "                idx += 1\n",
    "    \n",
    "    precision = 0\n",
    "    if count > 0:    \n",
    "        precision = float(correctCount) / count\n",
    "        \n",
    "    return precision\n",
    "\n",
    "predLabels, correctLabels = tag_dataset(test_batch)        \n",
    "pre_test, rec_test, f1_test= compute_f1(predLabels, correctLabels, idx2Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test-Data: Prec: 0.961, Rec: 0.961, F1: 0.961\n"
     ]
    }
   ],
   "source": [
    "print(\"Test-Data: Prec: %.3f, Rec: %.3f, F1: %.3f\" % (pre_test, rec_test, f1_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
